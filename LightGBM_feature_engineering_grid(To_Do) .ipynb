{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意(attention)！开始做前必读项！\n",
    "所有的代码一定要在这个文件里面编写，不要自己创建一个新的文件\n",
    "对于提供的数据集，不要改存储地方，也不要修改文件名和内容\n",
    "不要重新定义函数（如果我们已经定义好的），按照里面的思路来编写。当然，除了我们定义的部分，如有需要可以自行定义函数或者模块\n",
    "写完之后，重新看一下哪一部分比较慢，然后试图去优化。一个好的习惯是每写一部分就思考这部分代码的时间复杂度和空间复杂度，AI工程是的日常习惯！\n",
    "这次作业很重要，一定要完成！ 相信会有很多的收获！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关的包 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T01:31:15.400383Z",
     "start_time": "2020-07-05T01:31:07.815536Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "from bayes_opt import BayesianOptimization\n",
    "from gensim import models\n",
    "\n",
    "import jieba\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T01:31:18.227908Z",
     "start_time": "2020-07-05T01:31:18.222923Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 500  # 表示样本表示最大的长度,表示降维之后的维度\n",
    "sentence_max_length = 1500  # 表示句子/样本在降维之前的维度\n",
    "Train_features3, Test_features3, Train_label3, Test_label3 = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取训练好的词嵌入向量和文本文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyedVectors.load_word2vec_format(file path, binary)  \n",
    "file path: 词向量模型路径  \n",
    "binary: text format or binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo\n",
    "# 通过models.KeyedVectors加载预训练好的embedding\n",
    "root_path = '/home/user10000440/notespace/src/Embedding/models/'\n",
    "# fast_embedding = models.KeyedVectors.load(root_path + 'fasttext_model')\n",
    "fast_embedding = models.KeyedVectors.load_word2vec_format(root_path + 'fasttext_model_wv.bin', binary=True)\n",
    "w2v_embedding = models.KeyedVectors.load_word2vec_format(root_path + 'w2v_model_wv.bin', binary=True)\n",
    "\n",
    "print(\"fast_embedding输出词表的个数{},w2v_embedding输出词表的个数{}\".format(\n",
    "    len(fast_embedding.vocab.keys()), len(w2v_embedding.vocab.keys())))\n",
    "\n",
    "print(\"取词向量成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo\n",
    "# 读取train_clean.tsv ，test_clean.tsv训练集和测试集文件\n",
    "# hint: 通过pandas中的read_csv读取数据集\n",
    "data_path = '~/dataset/图书分类文本数据集/5813f3b1-617d-47ad-87e8-37a773a983af/file/归档/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train_clean.csv', sep='\\t')\n",
    "test = pd.read_csv(data_path + 'test_clean.csv', sep='\\t')\n",
    "print(\"读取数据完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将df中的label映射为数字标签并保存到labelIndex列中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelName = train.label.unique()  # 全部label列表\n",
    "labelIndex = list(range(len(labelName)))  # 全部label标签\n",
    "labelNameToIndex = dict(zip(labelName, labelIndex))  # label的名字对应标签的字典\n",
    "labelIndexToName = dict(zip(labelIndex, labelName))  # label的标签对应名字的字典\n",
    "train[\"labelIndex\"] = train.label.map(labelNameToIndex)\n",
    "test[\"labelIndex\"] = test.label.map(labelNameToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取 label embedding\n",
    "\n",
    "# label_cut = [jieba.cut(label.replace('/', '')) for label in labelName]\n",
    "\n",
    "# global fast_embedding, w2v_embedding\n",
    "\n",
    "# 每个label对应一个向量，这样做法导致严重过拟合，因为把label信息加入到训练数据里\n",
    "# label_w2v_embedding_list = []\n",
    "# for label in label_cut:\n",
    "#     query = [word for word in label]\n",
    "#     label_w2v_arr = np.array([w2v_embedding.get_vector(s)\n",
    "#                          for s in query if s in w2v_embedding.vocab.keys()])\n",
    "#     label_w2v_embedding_list.append(label_w2v_arr)\n",
    "    \n",
    "# def get_label_embedding(label_index):\n",
    "#     return label_w2v_embedding_list[label_index]\n",
    "\n",
    "label_cut = [jieba.cut(label.replace('/', '')) for label in labelName]\n",
    "\n",
    "global fast_embedding, w2v_embedding\n",
    "\n",
    "label_w2v_embedding_list = []\n",
    "for label in label_cut:\n",
    "    query = [word for word in label]\n",
    "    label_w2v_arr = np.array([w2v_embedding.get_vector(s)\n",
    "                         for s in query if s in w2v_embedding.vocab.keys()])\n",
    "    \n",
    "    result_avg = np.average(label_w2v_arr, axis = 0)\n",
    "    label_w2v_embedding_list.append([r for r in result_avg])\n",
    "    \n",
    "def get_label_embedding():\n",
    "    return np.array(label_w2v_embedding_list)\n",
    "\n",
    "# test\n",
    "# b = get_label_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cut(query):\n",
    "    '''\n",
    "    函数说明：该函数用于对输入的语句（query）按照空格进行切分\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 第一步：定义一个query_cut函数 将query按空格划分并返回，\n",
    "    # hint: 通过sqlit对query进行划分\n",
    "    query_list = query.split(' ')\n",
    "    return query_list\n",
    "\n",
    "# 第二步：然后train和test中的每一个样本都调用该函数，\n",
    "#      并将划分好的样本分别存储到train[\"queryCut\"]和test[\"queryCut\"]中\n",
    "train[\"queryCut\"] = train.text.apply(query_cut)\n",
    "test[\"queryCut\"] = test.text.apply(query_cut)\n",
    "print(\"切分数据完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['queryCut'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords.txt', \"r\") as f:\n",
    "    #ToDo\n",
    "    # 第一步：按行读取停用词文件\n",
    "    stopWords = [line.strip() for line in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_stop_word(wordList):\n",
    "    '''\n",
    "    函数说明：该函数用于去除输入样本中的存在的停用词\n",
    "    Return: 返回去除停用词之后的样本\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 第二步：去除每个样本中的停用词并返回新的样本\n",
    "    new_word_list = []\n",
    "    for index, word in enumerate(wordList):\n",
    "        if word not in stopWords:\n",
    "            new_word_list.append(word)\n",
    "    return new_word_list\n",
    "    \n",
    "train[\"queryCutRMStopWord\"] = train[\"queryCut\"].apply(rm_stop_word)\n",
    "# dev[\"queryCutRMStopWord\"] = dev[\"text\"].apply(rm_stop_word)\n",
    "test[\"queryCutRMStopWord\"] = test[\"queryCut\"].apply(rm_stop_word)\n",
    "print(\"去除停用词\")\n",
    "print(type(train[\"queryCutRMStopWord\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.signal import sepfir2d\n",
    "\n",
    "def functor_con2d(embedding_matrix, n, core_num):# 仿2维卷积\n",
    "    #print(\"functor_con2d:\", embedding_matrix.shape)\n",
    "    if (embedding_matrix.shape[0] < n):\n",
    "        print(\"functor_con2d return None\")\n",
    "        return None\n",
    "    \n",
    "    core_size = embedding_matrix.shape[1]\n",
    "    #core = np.ones(n * core_size) / (n * core_size)\n",
    "    \n",
    "    embedding_matrix_one = embedding_matrix.reshape(embedding_matrix.shape[0] * embedding_matrix.shape[1])\n",
    "    \n",
    "    result_all = []\n",
    "    for core_index in range(core_num):         \n",
    "        core = np.random.rand(core_size * n)        \n",
    "        result_ = []\n",
    "        for j in range(embedding_matrix.shape[0] - n + 1):\n",
    "            start_index = j * embedding_matrix.shape[1]\n",
    "            end_index = (n * embedding_matrix.shape[1]) + start_index\n",
    "\n",
    "            embedding_ = (embedding_matrix_one[start_index:end_index])\n",
    "\n",
    "            #print(\"embedding:\",embedding_)\n",
    "            #print(\"core:\", core)\n",
    "            x = np.convolve(embedding_, core, 'valid')\n",
    "            #print(\"con:\",x)\n",
    "\n",
    "            result_.append(x[0])\n",
    "            \n",
    "        result_all.append(result_)\n",
    "        \n",
    "    # print(\"functor_con2d end\")\n",
    "       \n",
    "    return result_all\n",
    "\n",
    "# test\n",
    "embedding_matrix_test = np.ones(4 * 2).reshape(4, 2)\n",
    "functor_con2d(embedding_matrix_test, 2, 3)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_embedding_with_windows(embedding_matrix):\n",
    "    '''\n",
    "    函数说明：`该函数用于获取在大小不同的滑动窗口(k=[2, 3, 4])， 然后进行平均或取最大操作。\n",
    "    参数说明：\n",
    "        - embedding_matrix：样本中所有词构成的词向量矩阵\n",
    "    return: result_list 返回拼接而成的一维词向量\n",
    "    '''\n",
    "    \n",
    "    result_list = []\n",
    "    # ToDo:\n",
    "    # 由于之前抽取的特征并没有考虑词与词之间交互对模型的影响，\n",
    "    # 对于分类模型来说，贡献最大的不一定是整个句子， 可能是句子中的一部分， 如短语、词组等等。 \n",
    "    # 在此基础上我们使用大小不同的滑动窗口(k=[2, 3, 4])， 然后进行平均或取最大操作。\n",
    "    k = [2, 3, 4]\n",
    "    for n in k:\n",
    "        x = functor_con2d(embedding_matrix, n, 3)\n",
    "        if x is None:\n",
    "            result_avg = np.zeros(1)\n",
    "            result_max = np.zeros(1)\n",
    "        else:\n",
    "            result_avg = np.average(x, axis = 1)\n",
    "            result_max = np.max(x, axis = 1)\n",
    "        \n",
    "        result_list = np.concatenate((result_list, result_avg, result_max), axis=0)\n",
    "        \n",
    "    return result_list\n",
    "\n",
    "\n",
    "# test\n",
    "embedding_matrix_test = np.ones(4 * 2).reshape(4, 2)\n",
    "result = Find_embedding_with_windows(embedding_matrix_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    #如果是列向量，则axis=0\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    s = x_exp / x_sum    \n",
    "    return s\n",
    "\n",
    "\n",
    "def Find_Label_embedding(word_matrix, label_embedding):\n",
    "    '''\n",
    "    函数说明：获取到所有类别的 label embedding， 与输入的 word embedding 矩阵相乘， 对其结果进行 softmax 运算，\n",
    "            对 attention score 与输入的 word embedding 相乘的结果求平均或者取最大\n",
    "            可以参考论文《Joint embedding of words and labels》获取标签空间的词嵌入\n",
    "    parameters:\n",
    "    -- example_matrix(np.array 2D): denotes the matrix of words embedding\n",
    "    -- embedding(np.array 2D): denotes the embedding of all label in data\n",
    "    return: (np.array 1D) the embedding by join label and word\n",
    "    '''\n",
    "    result_embedding=[]\n",
    "    # To_Do \n",
    "    # 第一步：基于consin相似度计算word embedding向量与label embedding之间的相似度\n",
    "    # 第二步：通过softmax获取注意力分布\n",
    "    # 第三步：将求得到的注意力分布与输入的word embedding相乘，并对结果进行最大化或求平均\n",
    "    word_label_matrix = np.dot(word_matrix, label_embedding)\n",
    "    \n",
    "    attention_score = softmax(word_label_matrix)\n",
    "    \n",
    "    attention_score_average = np.average(attention_score, axis=1)\n",
    "    result_avg = np.dot(attention_score_average, word_matrix)\n",
    "     \n",
    "    attention_score_max = np.max(attention_score, axis=1)\n",
    "    result_max = np.dot(attention_score_max, word_matrix)\n",
    "    \n",
    "   \n",
    "    result_embedding = np.concatenate((result_embedding, result_avg, result_max), axis=0)\n",
    "    \n",
    "    return result_embedding\n",
    "\n",
    "# test\n",
    "embedding_matrix_test = np.array([1 for i in range(80)]).reshape([10,8])\n",
    "label_embedding_test = np.array([1 for i in range(32)]).reshape([8,4])\n",
    "Find_Label_embedding(embedding_matrix_test, label_embedding_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_row_index = 0\n",
    "def sentence2vec(query):\n",
    "    '''\n",
    "    函数说明：联合多种特征工程来构造新的样本表示，主要通过以下三种特征工程方法\n",
    "            第一：利用word-embedding的average pooling和max-pooling\n",
    "            第二：利用窗口size=2，3，4对word-embedding进行卷积操作，然后再进行max/avg-pooling操作\n",
    "            第二：利用类别标签的表示，增加了词语和标签之间的语义交互，以此达到对词级别语义信息更深层次的考虑\n",
    "            另外，对于词向量超过预定义的长度则进行截断，小于则进行填充\n",
    "    参数说明：\n",
    "    - query:数据集中的每一个样本\n",
    "    return: 返回样本经过哦特征工程之后得到的词向量\n",
    "    '''\n",
    "    global max_length\n",
    "    arr = []\n",
    "    # 加载fast_embedding,w2v_embedding\n",
    "    global fast_embedding, w2v_embedding\n",
    "    fast_arr = np.array([fast_embedding.wv.get_vector(s)\n",
    "                         for s in query if s in fast_embedding.wv.vocab.keys()])\n",
    "    # 在fast_arr下滑动获取到的词向量\n",
    "    if len(fast_arr) > 0:\n",
    "        windows_fastarr = np.array(Find_embedding_with_windows(fast_arr))\n",
    "        result_attention_embedding = Find_Label_embedding(\n",
    "            fast_arr, get_label_embedding().T)\n",
    "    else:# 如果样本中的词都不在字典，则该词向量初始化为0\n",
    "        # 这里300表示训练词嵌入设置的维度为300\n",
    "        windows_fastarr = np.zeros(300) \n",
    "        result_attention_embedding = np.zeros(300)\n",
    "\n",
    "    fast_arr_max = np.max(np.array(fast_arr), axis=0) if len(\n",
    "        fast_arr) > 0 else np.zeros(300)\n",
    "    fast_arr_avg = np.mean(np.array(fast_arr), axis=0) if len(\n",
    "        fast_arr) > 0 else np.zeros(300)\n",
    "\n",
    "    fast_arr = np.hstack((fast_arr_avg, fast_arr_max))\n",
    "    # 将多个embedding进行横向拼接\n",
    "    arr = np.hstack((np.hstack((fast_arr, windows_fastarr)),\n",
    "                     result_attention_embedding))\n",
    "    global sentence_max_length\n",
    "    # 如果样本的维度大于指定的长度则需要进行截取或者拼凑,\n",
    "    result_arr = arr[:sentence_max_length] if len(arr) > sentence_max_length else np.hstack((\n",
    "        arr, np.zeros(int(sentence_max_length-len(arr)))))\n",
    "    return result_arr\n",
    "\n",
    "# def sentence2vec(query_label, query_label):\n",
    "#     '''\n",
    "#     函数说明：联合多种特征工程来构造新的样本表示，主要通过以下三种特征工程方法\n",
    "#             第一：利用word-embedding的average pooling和max-pooling\n",
    "#             第二：利用窗口size=2，3，4对word-embedding进行卷积操作，然后再进行max/avg-pooling操作\n",
    "#             第二：利用类别标签的表示，增加了词语和标签之间的语义交互，以此达到对词级别语义信息更深层次的考虑\n",
    "#             另外，对于词向量超过预定义的长度则进行截断，小于则进行填充\n",
    "#     参数说明：\n",
    "#     - query:数据集中的每一个样本\n",
    "#     return: 返回样本经过哦特征工程之后得到的词向量\n",
    "#     '''\n",
    "#     global g_row_index\n",
    "#     g_row_index += 1\n",
    "#     if g_row_index % 200 == 0:\n",
    "#         with open(\"/home/user10000440/notespace/output/log.txt\", 'w') as f:\n",
    "#             f.write(str(g_row_index))\n",
    "            \n",
    "#         print(\"g_row_index:\", g_row_index)\n",
    "        \n",
    "#     query = query_label[\"queryCutRMStopWord\"]\n",
    "#     label_index = query_label[\"labelIndex\"]\n",
    "    \n",
    "#     global max_length\n",
    "#     arr = []\n",
    "#     # 加载fast_embedding,w2v_embedding\n",
    "    \n",
    "#     global fast_embedding, w2v_embedding\n",
    "    \n",
    "#     fast_arr = np.array([fast_embedding.get_vector(s)\n",
    "#                          for s in query if s in fast_embedding.vocab.keys()])\n",
    "    \n",
    "#     # 在fast_arr下滑动获取到的词向量\n",
    "#     if len(fast_arr) > 0:\n",
    "#         windows_fastarr = np.array(Find_embedding_with_windows(fast_arr))\n",
    "        \n",
    "#         # find label embedding\n",
    "#         label_index_embedding = get_label_embedding()\n",
    "        \n",
    "#         result_attention_embedding = Find_Label_embedding(fast_arr, label_index_embedding.T)\n",
    "        \n",
    "#     else:# 如果样本中的词都不在字典，则该词向量初始化为0\n",
    "#         # 这里300表示训练词嵌入设置的维度为300\n",
    "#         windows_fastarr = np.zeros(300) \n",
    "#         result_attention_embedding = np.zeros(300)\n",
    "\n",
    "#     fast_arr_max = np.max(np.array(fast_arr), axis=0) if len(\n",
    "#         fast_arr) > 0 else np.zeros(300)\n",
    "#     fast_arr_avg = np.mean(np.array(fast_arr), axis=0) if len(\n",
    "#         fast_arr) > 0 else np.zeros(300)\n",
    "\n",
    "#     fast_arr = np.hstack((fast_arr_avg, fast_arr_max))\n",
    "#     # 将多个embedding进行横向拼接\n",
    "#     fast_win_arr = np.hstack((fast_arr, windows_fastarr))\n",
    "#     arr = np.hstack((fast_win_arr, result_attention_embedding))\n",
    "\n",
    "#     global sentence_max_length\n",
    "#     # 如果样本的维度大于指定的长度则需要进行截取或者拼凑,\n",
    "#     result_arr = arr[:sentence_max_length] if len(arr) > sentence_max_length else np.hstack((\n",
    "#         arr, np.zeros(int(sentence_max_length-len(arr)))))\n",
    "#     return result_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train_features, Test_features, Train_label, Test_label = Find_Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Dimension_Reduction(Train, Test):\n",
    "    '''\n",
    "    函数说明：该函数通过PCA算法对样本进行降维，由于目前维度不是特别搞高 ，可以选择不降维。\n",
    "    参数说明：\n",
    "    - Train: 表示训练数据集\n",
    "    - Test: 表示测试数据集\n",
    "    Return: 返回降维之后的数据样本\n",
    "    '''\n",
    "    global max_length\n",
    "    #  To_Do \n",
    "    # 特征选择，由于经过特征工程得到的样本表示维度很高，因此需要进行降维 max_length表示降维之后的样本最大的维度。\n",
    "    # 这里通过PCA方法降维\n",
    "    \n",
    "    print(\"Dimension_Reduction Train.shape:\", Train.shape)\n",
    "    pca = PCA(n_components=max_length)\n",
    "    pca_train = pca.fit_transform(Train) \n",
    "    pca_test = pca.fit_transform(Test) \n",
    "    print(\"Dimension_Reduction for PCA Train.shape:\", pca_train.shape)\n",
    "    return pca_train, pca_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Find_Embedding():\n",
    "    '''\n",
    "    函数说明：该函数用于获取经过特征工程之后的样本表示\n",
    "    Return:训练集特征数组(2D)，测试集特征数组(2D)，训练集标签数组（1D）,测试集标签数组（1D）\n",
    "    '''\n",
    "    print(\"获取样本表示中...\")\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    Train_features2 = min_max_scaler.fit_transform(\n",
    "        np.vstack(train[\"queryCutRMStopWord\"].apply(sentence2vec)))\n",
    "    Test_features2 = min_max_scaler.fit_transform(\n",
    "        np.vstack(test[\"queryCutRMStopWord\"].apply(sentence2vec)))\n",
    "    print(\"获取样本词表示完成\")\n",
    "    # 通过PCA对样本表示进行降维\n",
    "    Train_features2, Test_features2 = Dimension_Reduction(\n",
    "        Train=Train_features2, Test=Test_features2)\n",
    "    Train_label2 = train[\"labelIndex\"]\n",
    "    Test_label2 = test[\"labelIndex\"]\n",
    "    \n",
    "    # 通过上采样的方法从原始数据集中抽样出平衡的数据集\n",
    "    # X_resampled , y_resampled = SMOTE().fit_sample(Train_features2 , Train_label2)\n",
    "\n",
    "    print(\"加载训练好的词向量\")\n",
    "    print(\"Train_features.shape =\", Train_features2.shape)\n",
    "    print(\"Test_features.shape =\", Test_features2.shape)\n",
    "    print(\"Train_label.shape =\", Train_label2.shape)\n",
    "    print(\"Test_label.shape =\", Test_label2.shape)\n",
    "\n",
    "    return Train_features2, Test_features2, Train_label2, Test_label2\n",
    "\n",
    "# def Find_Embedding():\n",
    "#     '''\n",
    "#     函数说明：该函数用于获取经过特征工程之后的样本表示\n",
    "#     Return:训练集特征数组(2D)，测试集特征数组(2D)，训练集标签数组（1D）,测试集标签数组（1D）\n",
    "#     '''\n",
    "#     print(\"获取样本表示中...\")\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "#     print(\"获取样本表示中 Train_features2...\")\n",
    "#     Train_features2 = min_max_scaler.fit_transform(\n",
    "#         np.vstack(train[[\"queryCutRMStopWord\", \"labelIndex\"]].apply(sentence2vec, axis= 1)))\n",
    "    \n",
    "#     print(\"获取样本表示中 Test_features2...\")\n",
    "#     global g_row_index\n",
    "#     g_row_index = 0\n",
    "    \n",
    "#     Test_features2 = min_max_scaler.fit_transform(\n",
    "#         np.vstack(test[[\"queryCutRMStopWord\", \"labelIndex\"]].apply(sentence2vec, axis= 1)))\n",
    "    \n",
    "#     print(\"获取样本词表示完成\")\n",
    "#     # 通过PCA对样本表示进行降维\n",
    "#     Train_features2, Test_features2 = Dimension_Reduction(\n",
    "#         Train=Train_features2, Test=Test_features2)\n",
    "#     Train_label2 = train[\"labelIndex\"]\n",
    "#     Test_label2 = test[\"labelIndex\"]\n",
    "\n",
    "#     print(\"加载训练好的词向量\")\n",
    "#     print(\"Train_features.shape =\", Train_features2.shape)\n",
    "#     print(\"Test_features.shape =\", Test_features2.shape)\n",
    "#     print(\"Train_label.shape =\", Train_label2.shape)\n",
    "#     print(\"Test_label.shape =\", Test_label2.shape)\n",
    "\n",
    "#     return Train_features2, Test_features2, Train_label2, Test_label2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Predict(Train_label, Test_label, Train_predict_label, Test_predict_label, model_name):\n",
    "    '''\n",
    "    函数说明：直接输出训练集和测试在模型上的准确率\n",
    "    参数说明：\n",
    "        - Train_label: 真实的训练集标签（1D）\n",
    "        - Test_labelb: 真实的测试集标签（1D）\n",
    "        - Train_predict_label: 模型在训练集上的预测的标签(1D)\n",
    "        - Test_predict_label: 模型在测试集上的预测标签（1D）\n",
    "        - model_name: 表示训练好的模型\n",
    "    Return: None\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 通过调用metrics.accuracy_score计算训练集和测试集上的准确率\n",
    "    print(model_name+'_'+'Train accuracy %s' % str(np.sum(Train_label == Train_predict_label) / len(Train_label)))\n",
    "    # 输出测试集的准确率\n",
    "    print(model_name+'_'+'test accuracy %s' % str(np.sum(Test_label == Test_predict_label) / len(Test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "第一步：完成LGBMClassifier模型中参数  \n",
    "第二步：根据joblib.dump保存训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Grid_Train_model(Train_features, Test_features, Train_label, Test_label):\n",
    "    '''\n",
    "    函数说明：基于网格搜索优化的方法搜索模型最优参数，最后保存训练好的模型\n",
    "    参数说明：\n",
    "        - Train_features: 训练集特征数组（2D）\n",
    "        - Test_features: 测试集特征数组（2D）\n",
    "        - Train_label: 真实的训练集标签 (1D)\n",
    "        - Test_label: 真实的测试集标签（1D）\n",
    "    Return: None\n",
    "    '''\n",
    "    with open(\"/home/user10000440/notespace/output/log.txt\", 'w+') as f:\n",
    "        f.write(\"Grid_Train_model begin\")\n",
    "    \n",
    "    parameters = {\n",
    "        'max_depth': [5],\n",
    "        'learning_rate': [0.2],\n",
    "#         'n_estimators': [100, 500, 1000, 1500, 2000],\n",
    "#         'min_child_weight': [0, 2, 5, 10, 20],\n",
    "#         'max_delta_step': [0, 0.2, 0.6, 1, 2],\n",
    "#         'subsample': [0.6, 0.7, 0.8, 0.85, 0.95],\n",
    "#         'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "#         'reg_alpha': [0, 0.25, 0.5, 0.75, 1],\n",
    "#         'reg_lambda': [0.2, 0.4, 0.6, 0.8, 1],\n",
    "#         'scale_pos_weight': [0.2, 0.4, 0.6, 0.8, 1]\n",
    "    }\n",
    "    # 定义分类模型列表，这里仅使用LightGBM模型\n",
    "    models = [\n",
    "        lgb.LGBMClassifier(),\n",
    "    ]\n",
    "    # 遍历模型\n",
    "    for model in models:\n",
    "        model_name = model.__class__.  __name__\n",
    "        global gsearch\n",
    "        gsearch = GridSearchCV(\n",
    "            model, param_grid=parameters, scoring='accuracy', cv=3, n_jobs=3)\n",
    "        gsearch.fit(Train_features, Train_label)\n",
    "        # 输出最好的参数\n",
    "        print(\"Best parameters set found on development set:{}\".format(\n",
    "            gsearch.best_params_))\n",
    "    \n",
    "        with open(\"/home/user10000440/notespace/output/log.txt\", 'w+') as f:\n",
    "            f.write(\"Best parameters set found on development set:{}\".format(gsearch.best_params_))\n",
    "    \n",
    "    \n",
    "        Test_predict_label = gsearch.predict(Test_features)\n",
    "        Train_predict_label = gsearch.predict(Train_features)\n",
    "        Predict(Train_label, Test_label,\n",
    "                Train_predict_label, Test_predict_label, model_name)\n",
    "    # 保存训练好的模型\n",
    "    joblib.dump(gsearch, '/home/user10000440/notespace/output/lgb_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主函数,先求训练集和测试集的词向量，然后根据Grid搜索来找到最佳参数的分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_features, Test_features, Train_label, Test_label = Find_Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('/home/user10000440/notespace/output/train_features2.txt', Train_features)\n",
    "np.savetxt('/home/user10000440/notespace/output/Test_features2.txt', Test_features)\n",
    "np.savetxt('/home/user10000440/notespace/output/Train_label2.txt', Train_label)\n",
    "np.savetxt('/home/user10000440/notespace/output/Test_label2.txt', Test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Grid_Train_model(Train_features=Train_features, Test_features=Test_features,Train_label=Train_label, Test_label=Test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global gsearch\n",
    "\n",
    "Test_predict_label = gsearch.predict(Test_features)\n",
    "Train_predict_label = gsearch.predict(Train_features)\n",
    "Predict(Train_label, Test_label, Train_predict_label, Test_predict_label, \"model_name\")\n",
    "# 保存训练好的模型\n",
    "joblib.dump(gsearch, '/home/user10000440/notespace/output/lgb_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test_predict_label = gsearch.predict(Test_features)\n",
    "Train_predict_label = gsearch.predict(Train_features)\n",
    "Predict(Train_label, Test_label, Train_predict_label, Test_predict_label, \"model_name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
